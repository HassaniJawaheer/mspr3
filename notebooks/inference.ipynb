{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52cf3f3f-e637-4259-90e2-aed5ab25f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "import holidays\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class Eco2mixFeaturesMinute:\n",
    "    \"\"\"Adds selected external features to 30-min Eco2mix data.\"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.df[\"Datetime\"] = pd.to_datetime(self.df[\"Datetime\"])\n",
    "\n",
    "    def add_temperature(self, latitude=48.85, longitude=2.35):\n",
    "        self.df[\"Hour\"] = self.df[\"Datetime\"].dt.floor(\"h\")\n",
    "        start_date = self.df[\"Hour\"].min().strftime(\"%Y-%m-%d\")\n",
    "        end_date = self.df[\"Hour\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"hourly\": \"temperature\",\n",
    "            \"timezone\": \"Europe/Paris\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            temp_df = pd.DataFrame(data[\"hourly\"])\n",
    "            temp_df[\"Hour\"] = pd.to_datetime(temp_df[\"time\"])\n",
    "            temp_df.drop(columns=[\"time\"], inplace=True)\n",
    "            self.df = self.df.merge(temp_df, on=\"Hour\", how=\"left\")\n",
    "            self.df.drop(columns=[\"Hour\"], inplace=True)\n",
    "\n",
    "            logging.info(\"Temperature features added.\")\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"[API Error - Temperature] {e}\")\n",
    "\n",
    "    def add_sunshine(self, latitude=48.85, longitude=2.35):\n",
    "        self.df[\"Date\"] = self.df[\"Datetime\"].dt.date\n",
    "        start_date = min(self.df[\"Date\"]).strftime(\"%Y-%m-%d\")\n",
    "        end_date = max(self.df[\"Date\"]).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"daily\": \"sunshine_duration\",\n",
    "            \"timezone\": \"Europe/Paris\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            sun_df = pd.DataFrame(data[\"daily\"])\n",
    "            sun_df[\"Date\"] = pd.to_datetime(sun_df[\"time\"]).dt.date\n",
    "            sun_df.drop(columns=[\"time\"], inplace=True)\n",
    "            self.df = self.df.merge(sun_df, on=\"Date\", how=\"left\")\n",
    "            self.df.drop(columns=[\"Date\"], inplace=True)\n",
    "            logging.info(\"Sunshine features added.\")\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"[API Error - Sunshine] {e}\")\n",
    "\n",
    "    def add_weekday(self):\n",
    "        self.df[\"Date\"] = self.df[\"Datetime\"].dt.date\n",
    "        jours_fr = [\"lundi\", \"mardi\", \"mercredi\", \"jeudi\", \"vendredi\", \"samedi\", \"dimanche\"]\n",
    "        self.df[\"weekday\"] = pd.to_datetime(self.df[\"Date\"]).dt.dayofweek.apply(lambda x: jours_fr[x])\n",
    "        self.df.drop(columns=[\"Date\"], inplace=True)\n",
    "        logging.info(\"Weekday column added.\")\n",
    "\n",
    "    def add_month(self):\n",
    "        self.df[\"Date\"] = self.df[\"Datetime\"].dt.date\n",
    "        mois_fr = [\n",
    "            \"janvier\", \"février\", \"mars\", \"avril\", \"mai\", \"juin\",\n",
    "            \"juillet\", \"août\", \"septembre\", \"octobre\", \"novembre\", \"décembre\"\n",
    "        ]\n",
    "        self.df[\"month\"] = pd.to_datetime(self.df[\"Date\"]).dt.month.apply(lambda x: mois_fr[x - 1])\n",
    "        self.df.drop(columns=[\"Date\"], inplace=True)\n",
    "        logging.info(\"Month column added.\")\n",
    "\n",
    "    def add_season(self):\n",
    "        self.df[\"Date\"] = self.df[\"Datetime\"].dt.date\n",
    "\n",
    "        def get_season(month):\n",
    "            return (\n",
    "                \"hiver\" if month in [12, 1, 2] else\n",
    "                \"printemps\" if month in [3, 4, 5] else\n",
    "                \"été\" if month in [6, 7, 8] else\n",
    "                \"automne\"\n",
    "            )\n",
    "        self.df[\"season\"] = pd.to_datetime(self.df[\"Date\"]).dt.month.apply(get_season)\n",
    "        self.df.drop(columns=[\"Date\"], inplace=True)\n",
    "        logging.info(\"Season column added.\")\n",
    "\n",
    "    def add_vacation(self):\n",
    "        self.df[\"Date\"] = self.df[\"Datetime\"].dt.date\n",
    "        years = pd.to_datetime(self.df[\"Date\"]).dt.year.unique()\n",
    "        public_holidays = holidays.FR(years=years)\n",
    "\n",
    "        zone_vacations_by_year = {\n",
    "            \"winter\": (\"02-05\", \"03-07\"),\n",
    "            \"spring\": (\"04-09\", \"05-09\"),\n",
    "            \"summer\": (\"07-06\", \"08-31\"),\n",
    "            \"Toussaint\": (\"10-17\", \"11-02\"),\n",
    "            \"christmas\": (\"12-18\", \"12-31\")\n",
    "        }\n",
    "\n",
    "        vacations = set()\n",
    "        for year in years:\n",
    "            for start, end in zone_vacations_by_year.values():\n",
    "                vacations.update(pd.date_range(start=f\"{year}-{start}\", end=f\"{year}-{end}\").date)\n",
    "\n",
    "        self.df[\"is_vacation\"] = self.df[\"Date\"].apply(lambda d: int(d in public_holidays or d in vacations))\n",
    "        self.df.drop(columns=[\"Date\"], inplace=True)\n",
    "        logging.info(\"Vacation flag added.\")\n",
    "\n",
    "    def run(self, include=None):\n",
    "        include = include or []\n",
    "        if \"temperature\" in include:\n",
    "            self.add_temperature()\n",
    "        if \"sunshine\" in include:\n",
    "            self.add_sunshine()\n",
    "        if \"weekday\" in include:\n",
    "            self.add_weekday()\n",
    "        if \"month\" in include:\n",
    "            self.add_month()\n",
    "        if \"season\" in include:\n",
    "            self.add_season()\n",
    "        if \"vacation\" in include:\n",
    "            self.add_vacation()\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00cb577e-0db0-4817-9cbc-cd87e11f14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_df(path):\n",
    "    return pd.read_csv(path, sep=\"\\t\", encoding=\"latin1\", index_col=False, low_memory=False)\n",
    "\n",
    "class Eco2mixLivePreparer:\n",
    "    \"\"\"\n",
    "    Prepare recent eco2mix data for inference\n",
    "    \"\"\"\n",
    "    ECO2MIX_LIVE_URL = \"https://eco2mix.rte-france.com/download/eco2mix/eCO2mix_RTE_En-cours-TR.zip\"\n",
    "\n",
    "    def __init__(self, tmp_dir=\"eco2mix_inference\"):\n",
    "        self.tmp_dir = Path(tmp_dir)\n",
    "        self.tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def load_and_prepare(self, target_date: str) -> pd.DataFrame:\n",
    "        target_dt = pd.to_datetime(target_date)\n",
    "        start_dt = target_dt - timedelta(days=7)\n",
    "\n",
    "        logger.info(f\"Preparing data for inference on target date: {target_date}\")\n",
    "        logger.info(f\"Will extract data from {start_dt.strftime('%Y-%m-%d')} to {target_dt.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        # 1. Download and extract ZIP\n",
    "        zip_path = self.tmp_dir / \"live.zip\"\n",
    "        response = requests.get(self.ECO2MIX_LIVE_URL)\n",
    "        with open(zip_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        logger.info(f\"Downloaded Eco2mix live ZIP to {zip_path}\")\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(self.tmp_dir)\n",
    "        logger.info(f\"Extracted ZIP content to {self.tmp_dir}\")\n",
    "\n",
    "        # 2. Locate the .xls file\n",
    "        xls_files = list(self.tmp_dir.glob(\"*.xls\"))\n",
    "        if not xls_files:\n",
    "            raise FileNotFoundError(\"No .xls file found in the archive.\")\n",
    "        xls_path = xls_files[0]\n",
    "        logger.info(f\"Found XLS file: {xls_path.name}\")\n",
    "\n",
    "        # 3. Read and filter data\n",
    "        df = read_df(xls_path)\n",
    "        df = df.iloc[:-1]  # remove summary row\n",
    "        df = df[[\"Date\", \"Heures\", \"Consommation\"]].dropna()\n",
    "        df[\"Datetime\"] = pd.to_datetime(df[\"Date\"].astype(str) + \" \" + df[\"Heures\"], errors=\"coerce\")\n",
    "        df = df[[\"Datetime\", \"Consommation\"]].dropna()\n",
    "        df = df[df[\"Datetime\"].dt.minute.isin([0, 30])]\n",
    "        df = df[df[\"Datetime\"].between(start_dt, target_dt - timedelta(minutes=30))]\n",
    "        df = df.sort_values(\"Datetime\").reset_index(drop=True)\n",
    "\n",
    "        if len(df) < 288:\n",
    "            raise ValueError(f\"Missing data: only {len(df)} rows found, 288 expected.\")\n",
    "        logger.info(f\"Filtered to {len(df)} rows of 30-minute consumption data.\")\n",
    "\n",
    "        # 4. Enrich with external features\n",
    "        df_features = Eco2mixFeaturesMinute(df)\n",
    "        df = df_features.run(include=[\"temperature\", \"sunshine\", \"weekday\", \"month\", \"season\", \"vacation\"])\n",
    "        logger.info(\"Added external features to data.\")\n",
    "\n",
    "        logger.info(\"Data preparation complete.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c9d4ee-ec30-449b-bcda-66a766cb998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Preparing data for inference on target date: 2025-05-07\n",
      "INFO:__main__:Will extract data from 2025-04-30 to 2025-05-07\n",
      "INFO:__main__:Downloaded Eco2mix live ZIP to eco2mix_inference/live.zip\n",
      "INFO:__main__:Extracted ZIP content to eco2mix_inference\n",
      "INFO:__main__:Found XLS file: eCO2mix_RTE_En-cours-TR.xls\n",
      "INFO:__main__:Filtered to 336 rows of 30-minute consumption data.\n",
      "INFO:root:Temperature features added.\n",
      "INFO:root:Sunshine features added.\n",
      "INFO:root:Weekday column added.\n",
      "INFO:root:Month column added.\n",
      "INFO:root:Season column added.\n",
      "INFO:root:Vacation flag added.\n",
      "INFO:__main__:Added external features to data.\n",
      "INFO:__main__:Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "preparer = Eco2mixLivePreparer()\n",
    "df_input = preparer.load_and_prepare(\"2025-05-07\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1be737-9b75-4ef2-bed2-a66952de2acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "import pandas as pd\n",
    "\n",
    "def rebuild_reference_dataset(df: pd.DataFrame, params: dict) -> TimeSeriesDataSet:\n",
    "    \"\"\"\n",
    "    Rebuild the reference TimeSeriesDataSet using the same structure as during training.\n",
    "\n",
    "    Parameters:\n",
    "    - df: full input DataFrame with 'Datetime', 'Consommation', and all features\n",
    "    - params: dict of training parameters (same as used in training phase)\n",
    "\n",
    "    Returns:\n",
    "    - TimeSeriesDataSet object for inference use\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
    "    df = df.sort_values(\"Datetime\").reset_index(drop=True)\n",
    "    df[\"time_idx\"] = range(len(df))\n",
    "    df[\"series_id\"] = \"France\"\n",
    "\n",
    "    dataset = TimeSeriesDataSet(\n",
    "        df,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"Consommation\",\n",
    "        group_ids=[\"series_id\"],\n",
    "        max_encoder_length=params.get(\"max_encoder_length\", 288),\n",
    "        max_prediction_length=params.get(\"max_prediction_length\", 48),\n",
    "        time_varying_known_reals=params.get(\"known_reals\", []),\n",
    "        time_varying_unknown_reals=params.get(\"unknown_reals\", []),\n",
    "        time_varying_known_categoricals=params.get(\"known_categoricals\", []),\n",
    "        time_varying_unknown_categoricals=params.get(\"unknown_categoricals\", []),\n",
    "        static_reals=params.get(\"static_reals\", []),\n",
    "        static_categoricals=params.get(\"static_categoricals\", []),\n",
    "        target_normalizer=params.get(\"target_normalizer\", None),\n",
    "        add_relative_time_idx=params.get(\"add_relative_time_idx\", True),\n",
    "        add_target_scales=params.get(\"add_target_scales\", True),\n",
    "        add_encoder_length=params.get(\"add_encoder_length\", True)\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c269c4c4-ef8f-4517-9a2d-98623fb1a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "\n",
    "class TFTInference:\n",
    "    def __init__(self, model_ckpt_path, reference_dataset):\n",
    "        self.model = TemporalFusionTransformer.load_from_checkpoint(model_ckpt_path)\n",
    "        self.reference_dataset = reference_dataset\n",
    "\n",
    "    def predict(self, df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_input = df_input.copy()\n",
    "        df_input[\"Datetime\"] = pd.to_datetime(df_input[\"Datetime\"])\n",
    "        df_input = df_input.sort_values(\"Datetime\").reset_index(drop=True)\n",
    "        df_input[\"time_idx\"] = range(len(df_input))\n",
    "        df_input[\"series_id\"] = \"France\"\n",
    "\n",
    "        # Prepare dataset for prediction\n",
    "        pred_dataset = TimeSeriesDataSet.from_dataset(\n",
    "            self.reference_dataset,\n",
    "            df_input,\n",
    "            predict=True,\n",
    "            stop_randomization=True\n",
    "        )\n",
    "\n",
    "        # Run prediction\n",
    "        loader = pred_dataset.to_dataloader(train=False, batch_size=1)\n",
    "        predictions = self.model.predict(loader)\n",
    "\n",
    "        return {\"prediction\": predictions[0].detach().cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050dc55c-90d8-4a62-b4cd-9e115dbfc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Prepare input data\n",
    "preparer = Eco2mixLivePreparer()\n",
    "df_input = preparer.load_and_prepare(\"2025-05-07\")\n",
    "\n",
    "params = {\n",
    "    \"max_encoder_length\": 6*48,\n",
    "    \"max_prediction_length\": 48,\n",
    "    \"known_reals\": [\"time_idx\", \"temperature\", \"is_vacation\"],\n",
    "    \"unknown_reals\": [\"Consommation\", \"sunshine_duration\"],\n",
    "    \"known_categoricals\": [\"weekday\", \"month\", \"season\"],\n",
    "}\n",
    "\n",
    "# recreate the reference dataset\n",
    "reference_dataset = rebuild_reference_dataset(df_input, params)\n",
    "\n",
    "# Initialize engine\n",
    "model_path = \"lightning_logs/tft_model/version_14/Checkpoint/epoch=15-val_loss=684.98.ckpt\"\n",
    "engine = TFTInferenceEngine(model_path, reference_dataset)\n",
    "\n",
    "# Inference\n",
    "result = engine.predict(df_input)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
